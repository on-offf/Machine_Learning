{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "블로그글_추출_단어생성_알고리즘.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 출처 : https://wikidocs.net/159468\n",
        "# 모델 불러오기\n",
        "!pip install konlpy\n",
        "!pip install -U sentence-transformers\n",
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btx03AOoopl1",
        "outputId": "f7cc0bf3-3d7d-4d8e-f2e4-fc651bf94c81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.7/dist-packages (0.6.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.4.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.2.2)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.11.0+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.64.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.7)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.96)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.20.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.8.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.12.0+cu113)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.11.4)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.7.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.6.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.8.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트용 문구\n",
        "doc = \"\"\"\n",
        "제가 (중고) 신입으로 취업하기 위해서 넘어야 할 벽이 몇가지 있었는데, 그 중의 하나가 코딩테스트입니다.\n",
        "개인적으로 코딩테스트를 보지 않는 기업을 선호했으나, 최근 추세가 대부분의 회사는 코딩테스트가 필수적이더군요.\n",
        "그래서 인터넷에서 검색해본 공부방법을 따라해봤으나, 저하고는 그다지 맞지 않는다라는 느낌을 받았습니다.\n",
        "분명히 모든 일에는 효율적인 방향이 존재할 것이고, 인터넷에 올라와 있는 대부분의 방법은 효율적이지 않았습니다.\n",
        "\n",
        "코딩테스트가 핫해져서일까요? 최근 코딩테스트 관련 서적 두 종이 비슷한 시기에 파이썬이라는 언어를 가지고 출판되었습니다.\n",
        "그래서 초반에 구매를 선뜻 할 수는 없었습니다. 언제 파이썬 배워서 써먹지? 라는 생각이 먼저 들었습니다.\n",
        "며칠 고민을 하다가, 파이썬이면 그래도 범용적으로 많이 쓰이는 언어니까 배워두면 나쁘지 않을 것 같다는 생각을 가지고 구매를 했습니다.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "JWj8BHDQqRQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 품사 태깅 및 명사 추출\n",
        "okt = Okt()\n",
        "\n",
        "tokenized_doc = okt.pos(doc)\n",
        "tokenized_nouns = ' '.join([word[0] for word in tokenized_doc if word[1] == 'Noun'])\n",
        "\n",
        "print('품사 태깅 10개만 출력 :',tokenized_doc[:10])\n",
        "print('명사 추출 :',tokenized_nouns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-HuBPO-qv_x",
        "outputId": "6d9dff61-f508-42cf-f299-17a65c7221c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "품사 태깅 10개만 출력 : [('\\n', 'Foreign'), ('제', 'Noun'), ('가', 'Josa'), ('(', 'Punctuation'), ('중고', 'Noun'), (')', 'Punctuation'), ('신입', 'Noun'), ('으로', 'Josa'), ('취업', 'Noun'), ('하기', 'Verb')]\n",
            "명사 추출 : 제 중고 신입 취업 위해 벽 가지 그 중 하나 코딩 테스트 개인 코딩 테스트 보지 기업 선호 최근 추세 대부분 회사 코딩 테스트 필수 인터넷 검색 공부 방법 저 그다지 느낌 모든 일 효율 방향 존재 것 인터넷 대부분 방법 효율 코딩 테스트 핫 요 최근 코딩 테스트 관련 서적 두 종이 시기 파이썬 언어 가지 출판 초반 구매 선뜻 수 언제 파이썬 생각 먼저 며칠 고민 파이썬 범용 언어 것 생각 가지 구매\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 5개만 테스트 출력\n",
        "n_gram_range = (1, 1)\n",
        "\n",
        "count = CountVectorizer(ngram_range=n_gram_range).fit([tokenized_nouns])\n",
        "candidates = count.get_feature_names_out()\n",
        "\n",
        "print('단어 개수 :',len(candidates))\n",
        "print('단어 다섯개만 출력 :',candidates[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CU15mLkqyzm",
        "outputId": "b13a1726-fce9-4637-bd73-ba83c4ec9b4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 개수 : 43\n",
            "단어 다섯개만 출력 : ['가지' '개인' '검색' '고민' '공부']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 다국어 sbert 호출\n",
        "model = SentenceTransformer('sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens')\n",
        "doc_embedding = model.encode([doc])\n",
        "candidate_embeddings = model.encode(candidates)"
      ],
      "metadata": {
        "id": "i5pEK6RCrIdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 빈출 단어 5개 출력\n",
        "top_n = 5\n",
        "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
        "keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
        "print(keywords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9t548OyrOrV",
        "outputId": "2a311b2d-ae99-429f-d6c2-ee2a6458870b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['취업', '효율', '인터넷', '공부', '코딩']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mmr 알고리즘(다양한 종류의 단어 호출) 함수 생성\n",
        "def mmr(doc_embedding, candidate_embeddings, words, top_n, diversity):\n",
        "\n",
        "    # 문서와 각 키워드들 간의 유사도가 적혀있는 리스트\n",
        "    word_doc_similarity = cosine_similarity(candidate_embeddings, doc_embedding)\n",
        "\n",
        "    # 각 키워드들 간의 유사도\n",
        "    word_similarity = cosine_similarity(candidate_embeddings)\n",
        "\n",
        "    # 문서와 가장 높은 유사도를 가진 키워드의 인덱스를 추출.\n",
        "    # 만약, 2번 문서가 가장 유사도가 높았다면\n",
        "    # keywords_idx = [2]\n",
        "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
        "\n",
        "    # 가장 높은 유사도를 가진 키워드의 인덱스를 제외한 문서의 인덱스들\n",
        "    # 만약, 2번 문서가 가장 유사도가 높았다면\n",
        "    # ==> candidates_idx = [0, 1, 3, 4, 5, 6, 7, 8, 9, 10 ... 중략 ...]\n",
        "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
        "\n",
        "    # 최고의 키워드는 이미 추출했으므로 top_n-1번만큼 아래를 반복.\n",
        "    # ex) top_n = 5라면, 아래의 loop는 4번 반복됨.\n",
        "    for _ in range(top_n - 1):\n",
        "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
        "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
        "\n",
        "        # MMR을 계산\n",
        "        mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
        "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
        "\n",
        "        # keywords & candidates를 업데이트\n",
        "        keywords_idx.append(mmr_idx)\n",
        "        candidates_idx.remove(mmr_idx)\n",
        "\n",
        "    return [words[idx] for idx in keywords_idx]"
      ],
      "metadata": {
        "id": "qttqM3UvrfM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mmr 함수 실행\n",
        "mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CK7Tzmc2ru5e",
        "outputId": "02a508b0-e4fe-48fe-b3dd-ac1a6c894d45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['코딩', '인터넷', '공부', '효율', '기업']"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qZm7Lvb6rydK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}